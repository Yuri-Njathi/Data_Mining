{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scraping presents unique organisational challenges, one website's h1 tag contains a title of an article while another contains the web page title.\n",
    "\n",
    "You may be asked to scrape product prices from different websites, with the ultimate aim to be to compare product prices.\n",
    "\n",
    "Large scalable crawlers tend to fall into one of several patterns. Learning these patterns and and applicable situations to improve maintainability and robustness of the crawlers.\n",
    "\n",
    "We'll focus on web crawlers that collect a limited number of \"types\" of data from websites and store these data types as Python Objects that read and write from the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning and Defining Objects.\n",
    "It is easy to fall into the trap of trying to track all/almost all fields ,related to a product, that appear on multiple sites.This results in messy and hard to read datasets. The paradox of choice.\n",
    "\n",
    "One good scalable approach, when deciding which data to collect, is to ignore the websites and ask yourself, 'what do I need?' For example, when you want to compare product prices among multiple stores and track those product prices over time. Thus you'll need just enough information to uniquely identify the product (across the multiple stores) and that's it:\n",
    "- Product title\n",
    "- Manufacturer\n",
    "- Product ID no. (if available/relavant)\n",
    "\n",
    "The more detailed information e.g. price ,reviews etc would be specific to a particular store and thus stored separately.\n",
    "\n",
    "Ask yourself :\n",
    "- Will this info help with project goals? Is it nice to have or is it essential?\n",
    "\n",
    "- If it may help in future, how difficult will it be to go back and collect the data at a later time ?\n",
    "\n",
    "- Does it make logical sense to store the data within a particular object?\n",
    "\n",
    "If so, \n",
    "- is this data sparse or dense i.e. available on each site or every product e.g. colour\n",
    "\n",
    "- how large is the data?\n",
    "\n",
    "- If large data, will i need to contantly retrieve it or only on occasion?\n",
    "\n",
    "- How variable is the type of data?\n",
    "\n",
    "Let's say you'll do some meta analysis around product attributes and prices, the number of pages a book has etc you notice this data is scarce thus it may make sense to create a product type that looks like this:\n",
    "\n",
    "- Product title\n",
    "- Manufacturer\n",
    "- Products ID\n",
    "- Attiributes (optional list or dict)\n",
    "\n",
    "and an attribute type looking like :\n",
    "- attribute name\n",
    "- attribute value\n",
    "\n",
    "This flexibility to add new attributes over time without requiring a new data schema or rewrite code. When deciding how to store these attributes in the database, you can write JSON to the attribute table or store each attribute in a separate table with a product ID.\n",
    "\n",
    "For keeping track of the prices found for each product, you'll need the following :\n",
    "- Product ID\n",
    "- Store ID\n",
    "- Price \n",
    "- Date/Timestamp price was found at.\n",
    "\n",
    "What if the product's attributes modified the price of the product?\n",
    "For instance, some stores might charge more for large tshirts that smaller ones. For this you may consider splitting the single tshirt into separate product listings for each siz or creating a new item type to store information about instances of a product, containing these fields:\n",
    "- Product ID\n",
    "- Instance type (e.g. size fo shirt)\n",
    "\n",
    "and each price would look like :\n",
    "- Product instance ID\n",
    "- Store ID\n",
    "- Price\n",
    "- Date/Timestamp price was found at.\n",
    "\n",
    "These basic questions and logic are used when designing your Python objects, apply in almost every situation.\n",
    "\n",
    "\n",
    "The data model is the underlying foundation of all the code that uses it. It is vital to give serious thought and planning to what, you need to collect and how to store it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with different website layouts\n",
    "Google is able to extract useful data about a variety of websites, having no upfront knowledge about the website structure itself.\n",
    "\n",
    "Humans are able to identify the title and main content of a page, it is more difficult to get a bot to do something.\n",
    "\n",
    "Fortunately, in most cases of web crawling , the data is being collected from sites that you've used before, but a few, or a few dozen websites that are preselected by humans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
