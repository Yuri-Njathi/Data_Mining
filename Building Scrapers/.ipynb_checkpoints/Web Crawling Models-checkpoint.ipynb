{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scraping presents unique organisational challenges, one website's h1 tag contains a title of an article while another contains the web page title.\n",
    "\n",
    "You may be asked to scrape product prices from different websites, with the ultimate aim to be to compare product prices.\n",
    "\n",
    "Large scalable crawlers tend to fall into one of several patterns. Learning these patterns and and applicable situations to improve maintainability and robustness of the crawlers.\n",
    "\n",
    "We'll focus on web crawlers that collect a limited number of \"types\" of data from websites and store these data types as Python Objects that read and write from the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning and Defining Objects.\n",
    "It is easy to fall into the trap of trying to track all/almost all fields ,related to a product, that appear on multiple sites.This results in messy and hard to read datasets. The paradox of choice.\n",
    "\n",
    "One good scalable approach, when deciding which data to collect, is to ignore the websites and ask yourself, 'what do I need?' For example, when you want to compare product prices among multiple stores and track those product prices over time. Thus you'll need just enough information to uniquely identify the product (across the multiple stores) and that's it:\n",
    "- Product title\n",
    "- Manufacturer\n",
    "- Product ID no. (if available/relavant)\n",
    "\n",
    "The more detailed information e.g. price ,reviews etc would be specific to a particular store and thus stored separately.\n",
    "\n",
    "Ask yourself :\n",
    "- Will this info help with project goals? Is it nice to have or is it essential?\n",
    "\n",
    "- If it may help in future, how difficult will it be to go back and collect the data at a later time ?\n",
    "\n",
    "- Does it make logical sense to store the data within a particular object?\n",
    "\n",
    "If so, \n",
    "- is this data sparse or dense i.e. available on each site or every product e.g. colour\n",
    "\n",
    "- how large is the data?\n",
    "\n",
    "- If large data, will i need to contantly retrieve it or only on occasion?\n",
    "\n",
    "- How variable is the type of data?\n",
    "\n",
    "Let's say you'll do some meta analysis around product attributes and prices, the number of pages a book has etc you notice this data is scarce thus it may make sense to create a product type that looks like this:\n",
    "\n",
    "- Product title\n",
    "- Manufacturer\n",
    "- Products ID\n",
    "- Attiributes (optional list or dict)\n",
    "\n",
    "and an attribute type looking like :\n",
    "- attribute name\n",
    "- attribute value\n",
    "\n",
    "This flexibility to add new attributes over time without requiring a new data schema or rewrite code. When deciding how to store these attributes in the database, you can write JSON to the attribute table or store each attribute in a separate table with a product ID.\n",
    "\n",
    "For keeping track of the prices found for each product, you'll need the following :\n",
    "- Product ID\n",
    "- Store ID\n",
    "- Price \n",
    "- Date/Timestamp price was found at.\n",
    "\n",
    "What if the product's attributes modified the price of the product?\n",
    "For instance, some stores might charge more for large tshirts that smaller ones. For this you may consider splitting the single tshirt into separate product listings for each siz or creating a new item type to store information about instances of a product, containing these fields:\n",
    "- Product ID\n",
    "- Instance type (e.g. size fo shirt)\n",
    "\n",
    "and each price would look like :\n",
    "- Product instance ID\n",
    "- Store ID\n",
    "- Price\n",
    "- Date/Timestamp price was found at.\n",
    "\n",
    "These basic questions and logic are used when designing your Python objects, apply in almost every situation.\n",
    "\n",
    "\n",
    "The data model is the underlying foundation of all the code that uses it. It is vital to give serious thought and planning to what, you need to collect and how to store it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with different website layouts\n",
    "Google is able to extract useful data about a variety of websites, having no upfront knowledge about the website structure itself.\n",
    "\n",
    "Humans are able to identify the title and main content of a page, it is more difficult to get a bot to do something.\n",
    "\n",
    "Fortunately, in most cases of web crawling , the data is being collected from sites that you've used before, but a few, or a few dozen websites that are preselected by humans. this means that you don't need to use complicated algorithms or ml to detect which text on the page \"looks most lika a title\" or which is probably the \"main content\". `You determine what these elements are manually.`\n",
    "\n",
    "The most obvious approach is to write a separate web crawler or page parser for each website. Each might take in a URL, string, or BeautifulSoup object and return a Python object for the thing that was scraped.\n",
    "\n",
    "The following is an example of a Content Class (representing a piece of content on a website such as a news article) and two scraper functions that take in a BeautifulSoup Object and return an instance of Content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Delivering inclusive urban access: 3 uncomfortable truths\n",
      "URL : https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/\n",
      "\n",
      "\n",
      "The past few decades have been filled with a deep optimism about the role of cities and suburbs across the world. These engines of economic growth host a majority of world population, are major drivers of economic innovation, and have created pathways to opportunities for untold amounts of people.\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jeffrey Gutman\n",
      "Nonresident Senior Fellow - Global Economy and Development\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Adie Tomer\n",
      "Fellow - Metropolitan Policy Program\n",
      "\n",
      " Twitter\n",
      "AdieTomer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "But all is not well within our so-called Urban Century. Rapid urbanization, rising gentrification, concentrated poverty, and shortages of basic infrastructure have combined to create spatial inequity in cities and suburbs across the globe. The challenges of housing, moving, and employing so many people have led to longer travel times, rising housing costs, and unsustainable public spending. Moreover, policymakers are questioning traditional policies and approaches.\n",
      "The past couple years, we’ve led a project at Brookings—Moving to Access—that responds to these spatial challenges by promoting the idea of connecting people to opportunities as a new foundational principle for 21st century urban development. This principle of accessibility is meant to be a corollary to the natural questions we ask ourselves everyday about the communities where we live: Is this the best location to access employment? Are there nearby schools and health services? Is there a market in the neighborhood? How can I get from here to there? Such choices are valid for those with sufficient income. But what about those with more limited resources and thus choices in terms of affordable housing and affordable transport?\n",
      "While economists, planners, and engineers have promoted accessibility for decades, the concept is more often found in textbooks than formal urban policies. In the first stage of this project, we worked with a team of experts to determine what has stalled practical implementation of appropriate policies and practices? “Delivering Inclusive Access,” a report of this initial work, offers a synthesis of what we found and where we believe researchers, policymakers, and practitioners can take this work next. The paper found three central challenges.\n",
      "The fallacy of the single indicator\n",
      "The current transport regime’s approach to measurement is one of outward elegance: The dominant pursuit is speed, and the primary way to measure it is congestion (or what slows us down). Many have come to label this approach a pursuit of “mobility.” It is seen through different, but often singular, measures of how congestion affects a specific roadway. Such singular measures are easily interpreted by policymakers and civil society and can be translated directly into economic analysis of related investments through timesavings. They also conveniently serve such purposes as the internationally agreed-upon Sustainable Development Goals. Yet they actually don’t answer the fundamental question of who can reach where, in how much time, and at what cost.\n",
      "\n",
      "\n",
      "Related Content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Delivering inclusive access\n",
      "\n",
      "Jeffrey Gutman, Adie Tomer, Joseph W. Kane, Nirav Patel, and Ranjitha Shivaram\n",
      "August 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Measuring performance: Accessibility metrics in metropolitan regions around the world\n",
      "\n",
      "Geneviève Boisjoly and Ahmed El-Geneidy\n",
      "August 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Is better access key to inclusive cities?\n",
      "\n",
      "Jeffrey Gutman and Nirav Patel\n",
      "Wednesday, October 5, 2016\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accessibility measures can answer those questions, but not through any one measure. First, the variable social, economic, and political contexts related to access mean searching for a single magical indicator is counterintuitive. For example, a wealthy, automobile-centric region like Dallas, Texas, may have very different measurable goals than a denser, poorer region like Dar es Salaam, Tanzania. Second, academic literature is now rife with such complex measures that it could be difficult to communicate their methodology and results with practitioners. The development of a suite of indicators could offer a menu for policymakers and practitioners to judge accessibility based on local objectives, local conditions, and local capacity.\n",
      "The danger of excessive localization\n",
      "Decentralization and empowering local communities is fast becoming a mantra of governance experts across the world, from development practitioners at institutions like the World Bank to city-focused theorists. And for good reason: delegating policy design and fiscal authority directly to the local level helps ensure policies and practices respond to local needs and desires. Yet as urban areas spillover into contiguous and often numerous municipalities, local independence can introduce certain challenges, especially relating to social and environmental externalities. When it comes to transportation and land development, interests of one municipality are often different from its neighbors. And these divergent development goals can exacerbate accessibility challenges within growing regions, spreading people, housing jobs, and other activities further from one another.\n",
      "Addressing spatial inequities in land use and real estate markets require a broader approach to horizontal governance. While there are examples of metropolitan transport authorities, there is less willingness to consider metropolitan or horizontal governance of land use and fiscal policies. For example, should housing be coordinated across an entire region?\n",
      "Countries with a more centralized top down approach to governance, such as France and Germany, have greater ability to formulate metropolitan governance than more decentralized countries such as the U.S. This is not to say there is a one-size-fits-all approach, but there is an opportunity to test different solutions within different governance contexts, comparing how effective each model is to promote spatial inclusivity.\n",
      "The finance community is missing in action\n",
      "Financing is a central topic in infrastructure circles. As maintenance bills from the automobile era come due, populations continue to grow, and fiscal budgets are tight, how can urban areas afford to build enough infrastructure to support future economic growth? In response, new approaches are evolving in fiscal instruments, such as value capture and private-public partnerships. Missing in these discussions, however, are the implications for inclusive access.\n",
      "We conducted a multi-decade review of past academic literature on access and found that there is no clear substantive discussion of accessibility from a fiscal perspective. While urban transport and land use professionals clearly recognize their interrelationship in achieving inclusive accessibility, at least in theory, the fiscal and finance professionals generally ignore the implications of their instruments with regard to inclusivity. The multilateral development banks and their economic evaluations have ignored the distributive impacts until very recently. And the efforts of some countries to incorporate measures through multi-criteria analysis have had limited impact.\n",
      "This gap must be resolved in any effort toward inclusive urban development. There is little doubt that fiscal approaches must carefully assess who ultimately pays and that alternative finance instruments should be adapted to foster access for all.\n",
      "Going forward\n",
      "Our research confirms that there are enormous opportunities to advance accessibility theory into practice. At this point, what is desperately needed is to launch a range of case studies that deal with these issues and challenges under different geographic, governance, and economic contexts. The good news is that many initiatives are already underway, and more robust communication channels and technology can support such efforts. In Chicago, researchers created an online platform to visually explore accessibility by location. In Bogota, researchers evaluated how affordability is a key principle of access. And in Cairo and Kigali, researchers used open tools to achieve new insights for accessibility. Sharing the results of these case studies could lead to a new level of cross-disciplinary approaches to improve accessibility and lessen the effects of spatial inequity.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Men Who Want to Live Forever\n",
      "URL: https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "class Content:\n",
    "        def __init__(self, url, title, body):\n",
    "            self.url = url\n",
    "            self.title = title\n",
    "            self.body = body\n",
    "def getPage(url):\n",
    "    req = requests.get(url)\n",
    "    return BeautifulSoup(req.text,'html.parser')\n",
    "def scrapeNYTimes(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find(\"h1\").text\n",
    "    lines = bs.find_all(\"p\", {\"class\":\"story-content\"})\n",
    "    body = '\\n'.join([line.text for line in lines])\n",
    "    return Content(url, title, body)\n",
    "def scrapeBrookings(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find(\"h1\").text\n",
    "    body = bs.find(\"div\",{\"class\",\"post-body\"}).text\n",
    "    return Content(url, title, body)\n",
    "\n",
    "url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\n",
    "\n",
    "content = scrapeBrookings(url)\n",
    "print('Title : {}'.format(content.title))\n",
    "print('URL : {}\\n'.format(content.url))\n",
    "print(content.body)\n",
    "\n",
    "url = 'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'\n",
    "content = scrapeNYTimes(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "print(content.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In new scraper functions for additional news sites, one will notice that the sites parsing function:\n",
    "    - Selects the title element and extracts the text for the title.\n",
    "    - Selects the main content of the article.\n",
    "    - Selects other content items as needed.\n",
    "    - Returns a Content object instantiated with the strings found previously.\n",
    "    \n",
    "The only real site-dependent variables here are the CSS selectors used to obtain each piece of information. BeautifulSoup’s find and find_all functions take in a tag string and a dictionary of key/value attributes . These are passed into arguments in as parameters that define the structure of the site itself and the location of the target data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things more convinient, dealing with all of these tag arguments and key/value pairs, you can use the BeautifulSoup select function with a single string CSS selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"\n",
    "    Common base class for all articles/pages\n",
    "    \"\"\"\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print(\"URL: {}\".format(self.url))\n",
    "        print(\"TITLE: {}\".format(self.title))\n",
    "        print(\"BODY:\\n{}\".format(self.body))\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    Contains information about website structure\n",
    "    \"\"\"\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website class does not store information collected from individual pages but store info on how to collect that data.\n",
    "\n",
    "It doesn't store the Title page \"My Page Title\", it stores the string tag h1 that indicates where the titles can be found. This is why the class is called Website not Content.\n",
    "\n",
    "Using the above content and Website classes you can then write a Crawler to scrape the title and content of any URL that is provided for a given web page from a given website:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Crawler:\n",
    "    def getPage(self,url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text,'html.parser')\n",
    "    def safeGet(self,pageObj, selector):\n",
    "        '''\n",
    "        Utitlity func used to get a content string from a BeautifulSoup object\n",
    "        and a selector. Returns an empty string if no object is found for the given\n",
    "        selector.\n",
    "        '''\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0 :\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "    def parse(self,site,url):\n",
    "        \"\"\"\n",
    "        Extract content from a given page URL\n",
    "        \"\"\"\n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = self.safeGet(bs,site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(url, title, body)\n",
    "                #content.print()\n",
    "                print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Content object at 0x7f616bb47c50>\n"
     ]
    }
   ],
   "source": [
    "# Define the website objects ad kicks off the process : \n",
    "crawler = Crawler()\n",
    "siteData = [\n",
    "['O\\'Reilly Media', 'http://oreilly.com',\n",
    "'h1', 'section#product-description'],\n",
    "['Reuters', 'http://reuters.com', 'h1',\n",
    "'div.StandardArticleBody_body_1gnLA'],\n",
    "['Brookings', 'http://www.brookings.edu',\n",
    "'h1', 'div.post-body'],\n",
    "['New York Times', 'http://nytimes.com',\n",
    "'h1', 'p.story-content']\n",
    "]\n",
    "\n",
    "websites = []\n",
    "for row in siteData:\n",
    "    websites.append(Website(row[0],row[1],row[2],row[3]))\n",
    "\n",
    "crawler.parse(websites[0], 'http://shop.oreilly.com/product/'\\\n",
    "'0636920028154.do')\n",
    "crawler.parse(websites[1], 'http://www.reuters.com/article/'\\\n",
    "'us-usa-epa-pruitt-idUSKBN19W2D0')\n",
    "crawler.parse(websites[2], 'https://www.brookings.edu/blog/'\\\n",
    "'techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/')\n",
    "crawler.parse(websites[3], 'https://www.nytimes.com/2018/01/'\\\n",
    "'28/business/energy-environment/oil-boom.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new method might not seem remarkably simpler than writing a new Python function for each new website at first glance, imagine what happens when you go from a system with 4 website sources to a system with 20/200.\n",
    "\n",
    "Of course, the downside is that you are giving up a certain amount of flexibility. In\n",
    "the first example, each website gets its own free-form function to select and parse\n",
    "HTML however necessary, in order to get the end result. In the second example, each\n",
    "website needs to have a certain structure in which fields are guaranteed to exist, data\n",
    "must be clean coming out of the field, and each target field must have a unique and\n",
    "reliable CSS selector.\n",
    "However, I believe that the power and relative flexibility of this approach more than\n",
    "makes up for its real or perceived shortcomings.\n",
    "\n",
    "The next section covers specific\n",
    "applications and expansions of this basic template so that you can, for example, deal with missing fields, collect different types of data, crawl only through specific parts of a website, and store more-complex information about pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring Crawlers\n",
    "\n",
    "When scraping it is necessary to use the methods of crawling through webstes and finding new pages in an automated way.\n",
    "\n",
    "This shows how to incorporate these methods into a well-structured and expandable website crawler that can gather links and discover dta in an automated way. We'll go through 3 basic web crwlers : they apply to the majority of situations. Ryan wishes these structures inspire you to create elegant and robust crawler design.\n",
    "\n",
    "These methods are : \n",
    "- Crawling through search\n",
    "- Crawling through links.\n",
    "- crawling multiple page types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawling sites through search.\n",
    "\n",
    "One of the easiest ways.\n",
    "\n",
    "Although the process of searching a website for a keyword or topic and collecting\n",
    "a list of search results seems difficult, it is easy because : \n",
    "    \n",
    "- Most sites retrieve a list of search results for a particular topic by passing that topic as a string trough a\n",
    "parameter in the URL. \n",
    "For example: http://example.com?search=myTopic. where \"http://example.com?search=\" can be saved as an object and the topic simply appended to it.\n",
    "\n",
    "- After searching most sites will receive the results as an easily identifiable \n",
    "list of links, usually ith a convinient surrounding tag such as <span\n",
    "class=\"result\"> , the exact format of which can also be stored \n",
    "as a property of the website object. \n",
    "\n",
    "-Each result link is either a relative URL (\"/articles/page.html\")   or an absolute URL (\"http ://exampl e.c om/articles/page.html\"),   store either as a property of the Website object.\n",
    "\n",
    "- Locate and normalize the URLs on the search page, you've successfully reduced the problem to the example in the previous section. Extracting data from a given page, given a website format.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation\n",
    "The `Content class` is much the same. Adding the URL property to keep track of content found.\n",
    "\n",
    "The `Website Class` has a few new properties, the `SearchUrl` defines where you should go to get search results if you append the topic you are looking for. The `resultingListing` defines the box that holds info about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "    def __init__(self, topic, url, title, body):\n",
    "        self.topic = topic\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.url = url\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print(\"New article found for topic: {}\".format(self.topic))\n",
    "        print(\"TITLE: {}\".format(self.title))\n",
    "        print(\"BODY:\\n{}\".format(self.body))\n",
    "        print(\"URL: {}\".format(self.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
