{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scraping presents unique organisational challenges, one website's h1 tag contains a title of an article while another contains the web page title.\n",
    "\n",
    "You may be asked to scrape product prices from different websites, with the ultimate aim to be to compare product prices.\n",
    "\n",
    "Large scalable crawlers tend to fall into one of several patterns. Learning these patterns and and applicable situations to improve maintainability and robustness of the crawlers.\n",
    "\n",
    "We'll focus on web crawlers that collect a limited number of \"types\" of data from websites and store these data types as Python Objects that read and write from the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning and Defining Objects.\n",
    "It is easy to fall into the trap of trying to track all/almost all fields ,related to a product, that appear on multiple sites.This results in messy and hard to read datasets. The paradox of choice.\n",
    "\n",
    "One good scalable approach, when deciding which data to collect, is to ignore the websites and ask yourself, 'what do I need?' For example, when you want to compare product prices among multiple stores and track those product prices over time. Thus you'll need just enough information to uniquely identify the product (across the multiple stores) and that's it:\n",
    "- Product title\n",
    "- Manufacturer\n",
    "- Product ID no. (if available/relavant)\n",
    "\n",
    "The more detailed information e.g. price ,reviews etc would be specific to a particular store and thus stored separately.\n",
    "\n",
    "Ask yourself :\n",
    "- Will this info help with project goals? Is it nice to have or is it essential?\n",
    "\n",
    "- If it may help in future, how difficult will it be to go back and collect the data at a later time ?\n",
    "\n",
    "- Does it make logical sense to store the data within a particular object?\n",
    "\n",
    "If so, \n",
    "- is this data sparse or dense i.e. available on each site or every product e.g. colour\n",
    "\n",
    "- how large is the data?\n",
    "\n",
    "- If large data, will i need to contantly retrieve it or only on occasion?\n",
    "\n",
    "- How variable is the type of data?\n",
    "\n",
    "Let's say you'll do some meta analysis around product attributes and prices, the number of pages a book has etc you notice this data is scarce thus it may make sense to create a product type that looks like this:\n",
    "\n",
    "- Product title\n",
    "- Manufacturer\n",
    "- Products ID\n",
    "- Attiributes (optional list or dict)\n",
    "\n",
    "and an attribute type looking like :\n",
    "- attribute name\n",
    "- attribute value\n",
    "\n",
    "This flexibility to add new attributes over time without requiring a new data schema or rewrite code. When deciding how to store these attributes in the database, you can write JSON to the attribute table or store each attribute in a separate table with a product ID.\n",
    "\n",
    "For keeping track of the prices found for each product, you'll need the following :\n",
    "- Product ID\n",
    "- Store ID\n",
    "- Price \n",
    "- Date/Timestamp price was found at.\n",
    "\n",
    "What if the product's attributes modified the price of the product?\n",
    "For instance, some stores might charge more for large tshirts that smaller ones. For this you may consider splitting the single tshirt into separate product listings for each siz or creating a new item type to store information about instances of a product, containing these fields:\n",
    "- Product ID\n",
    "- Instance type (e.g. size fo shirt)\n",
    "\n",
    "and each price would look like :\n",
    "- Product instance ID\n",
    "- Store ID\n",
    "- Price\n",
    "- Date/Timestamp price was found at.\n",
    "\n",
    "These basic questions and logic are used when designing your Python objects, apply in almost every situation.\n",
    "\n",
    "\n",
    "The data model is the underlying foundation of all the code that uses it. It is vital to give serious thought and planning to what, you need to collect and how to store it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with different website layouts\n",
    "Google is able to extract useful data about a variety of websites, having no upfront knowledge about the website structure itself.\n",
    "\n",
    "Humans are able to identify the title and main content of a page, it is more difficult to get a bot to do something.\n",
    "\n",
    "Fortunately, in most cases of web crawling , the data is being collected from sites that you've used before, but a few, or a few dozen websites that are preselected by humans. this means that you don't need to use complicated algorithms or ml to detect which text on the page \"looks most lika a title\" or which is probably the \"main content\". `You determine what these elements are manually.`\n",
    "\n",
    "The most obvious approach is to write a separate web crawler or page parser for each website. Each might take in a URL, string, or BeautifulSoup object and return a Python object for the thing that was scraped.\n",
    "\n",
    "The following is an example of a Content Class (representing a piece of content on a website such as a news article) and two scraper functions that take in a BeautifulSoup Object and return an instance of Content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "    def getPage(url):\n",
    "        req = requests.get(url)\n",
    "        return BeautifulSoup(req.text,'html.parser')\n",
    "    \n",
    "    def scrapeNYTimes(url):\n",
    "        bs = getPage(url)\n",
    "        title = bs.find(\"h1\").text\n",
    "        lines = bs.find_all(\"p\", {\"class\":\"story-content\"})\n",
    "        body = '\\n'.join([line.text for line in lines])\n",
    "        return Content(url, title, body)\n",
    "    def scrapeBrookings(url):\n",
    "        bs = getPage(url)\n",
    "        title = bs.find(\"h1\").text\n",
    "        body = bs.find(\"div\",{\"class\",\"post-body\"}).text\n",
    "        return Content(url, title, body)\n",
    "    \n",
    "    url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\n",
    "    \n",
    "    content = scrapeBrookings(url)\n",
    "    print('Title : {}'.format(content.title))\n",
    "    print('URL : {}\\n'.format(content.url))\n",
    "    print(content.body)\n",
    "    \n",
    "    url = 'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'\n",
    "    content = scrapeNYTimes(url)\n",
    "    print('Title: {}'.format(content.title))\n",
    "    print('URL: {}\\n'.format(content.url))\n",
    "    print(content.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In new scraper functions for additional news sites, one will notice that the sites parsing function:\n",
    "    - Selects the title element and extracts the text for the title.\n",
    "    - Selects the main content of the article.\n",
    "    - Selects other content items as needed.\n",
    "    - Returns a Content object instantiated with the strings found previously.\n",
    "    \n",
    "The only real site-dependent variables here are the CSS selectors used to obtain each piece of information. BeautifulSoupâ€™s find and find_all functions take in a tag string and a dictionary of key/value attributes . These are passed into arguments in as parameters that define the structure of the site itself and the location\n",
    "of the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
