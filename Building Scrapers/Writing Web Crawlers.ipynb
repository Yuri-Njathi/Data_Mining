{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real World Applications.\n",
    "Multiple pages and multiple sites.\n",
    "Web crawlers crawl across the web. At Their core is an element called recursion. They must retrieve page contents for a URL, examine that page for another URL and retrieve that page ad infinitum.\n",
    "\n",
    "The scrapers used in the previous examples work great in situations where all the data you need is on a single page. With web crawlers, you must be extremely conscientious of how much bandwidth you are using and make every effort to determine whether there's a way to make the target server's load easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transversing a Single Domain.\n",
    "There these games called Six Degrees of Wikipedia and Six Degrees of Kevin Bacon. The object of the games is to link tow unlikely subjects by a chain containing no more than six total including the two original subjects:\n",
    "For example, Eric Idle appeared in Dudley Do-Right with Brendan Fraser, who\n",
    "appeared in The Air I Breathe with Kevin Bacon. 1 In this case, the chain from Eric Idle\n",
    "to Kevin Bacon is only three subjects long.\n",
    "\n",
    "\n",
    "In this section, you’ll begin a project that will become a Six Degrees of Wikipedia sol‐\n",
    "ution finder: You’ll be able to take the Eric Idle page and find the fewest number of\n",
    "link clicks that will take you to the Kevin Bacon page. (Sound Cool Right?)\n",
    "\n",
    "\n",
    "##### But What About Wikipedia’s Server Load?\n",
    "According to the Wikimedia Foundation (the parent organization behind Wikipedia),\n",
    "the site’s web properties receive approximately 2,500 hits per second, with more than\n",
    "99% of them to the Wikipedia domain (see the “Traffic Volume” section of the “Wiki‐\n",
    "media in Figures” page). Because of the sheer volume of traffic, your web scrapers are\n",
    "unlikely to have any noticeable impact on Wikipedia’s server load. However, if you\n",
    "run the code samples in this book extensively, or create your own projects that scrape\n",
    "Wikipedia, I encourage you to make a tax-deductible donation to the Wikimedia\n",
    "Foundation—not just to offset your server load, but also to help make education\n",
    "resources available for everyone else.\n",
    "Also keep in mind that if you plan on doing a large project involving data from Wiki‐\n",
    "pedia, you should check to make sure that data isn’t already available from the Wiki‐\n",
    "pedia API. Wikipedia is often used as a website to demonstrate scrapers and crawlers\n",
    "because it has a simple HTML structure and is relatively stable. However, its APIs\n",
    "often make this same data more efficiently accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a Python Script that retrieves an arbitrary Wikipedia Page and Produces a list of links on that page.\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
